---
title: "Modelling Frequency2"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
library(tidyverse, quietly = T)
library(caret, quietly = T)
library(DMwR, quietly = T)
library(ROSE, quietly = T)
library(viridis, quietly = T)
library(DAAG, quietly = T)
```

# Predicting Condition

We use downsampling to balance the cases. We set up a number of different models:

* There are 7 data subsets, 1 for each frequency domain section and 1 extra using the frequency domain portion with the largest portion of data

* For each data subset, we predict condition using Logistic regression, XGBoost, RandomForest, SVM, and a basic Neural Network.

## Which Variables Have Enough Data?

```{r, include=FALSE}
#save participants who qualify with both episode and control
pp = c(201, 202, 207, 214, 215, 217, 218, 219, 226, 228)
```

```{r}
#how much data is available in each section?
features_output = read.csv("../DataOutputs/dat_out20181009.csv")%>%
  as.tibble()
features_output%>%
  filter(ID %in% pp)%>%
  #filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(Avg_niHR:LFHF_6)%>%
  summarise_at(.vars = vars(Avg_niHR:LFHF_6), funs(sum(!is.na(.))))%>%
  gather("variable", "Rows")%>%
  arrange(-Rows)
```

```{r}
features_output%>%
  filter(ID %in% pp)%>%
  #filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(Avg_niHR:LFHF_6)%>%
  naniar::vis_miss()
```

# Machine Learning

## Setup
```{r}
# Here we set the train and test set
set.seed(3)
mydat = features_output%>%
  filter(ID %in% pp)%>%
  #filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y, Avg_niHR:LFHF_6)%>%
  rename(Class = Y)%>%
  mutate(Class = as.factor(Class))

#set up a list of unnamed data sets based on variable names
# loop through list and select variables dynamically
# partition at 70%
# downsample training set

datasets = list()

for(n in 1:6){
  
  temp_dat = mydat%>%
    select(ID, Class, Class, Avg_niHR:Avg_HR, contains(as.character(n)))%>%
    filter(complete.cases(.))%>%
    group_by(ID, Class)%>%
    nest()%>%
    mutate(rows = purrr::map(data, nrow))%>%
    unnest(rows)%>%
    group_by(ID)%>%
    mutate(rows = min(rows))%>%
    ungroup()%>%
    mutate(samp = purrr::map2(data, rows, sample_n))%>%
    select(-data, -rows)%>%
    unnest(samp)%>%
    select(-ID)
    
  
   ind = createFolds(y = temp_dat$Class, k=4, list=TRUE, returnTrain = FALSE)
   
   datasets[[n]] = list(data = temp_dat, folds = ind)
    
}

#statistically significant diff variables
temp_dat = mydat%>%
  select(Class, Avg_niHR, End_niHR, Avg_HR, HF_4, LF_3)%>%
  filter(complete.cases(.))

ind = createFolds(y = temp_dat$Class, k=4, list=TRUE, returnTrain = FALSE)

datasets[[7]] = list(temp_dat, ind)
```

## Describing the ML Data

Here are the number of rows of the datasets (note that folds are identical):
```{r}

datasets[[1]]%>%
  
data.frame(do.call(cbind, datasets[[1]]$folds))
  
# getTrainTest = function(list_item){
#   
#   train = list_item$f1$train%>%
#     mutate(Set="Train")
#   test = list_item$f1$test%>%
#     mutate(Set="Test")
#   
#   rbind(train,test)%>%
#     group_by(Set, Class)%>%
#     count()%>%
#     return()
# }
# 
# descriptives = lapply(datasets, getTrainTest)
# descriptives
```

```{r}
CaretGlm = function(datalist){
  
  summaries = list()
  predictions = list()
  
  for(fold in 1:4){
    
    model = train(Class ~ .,
            method = "glm",
            family = binomial,
            data = datalist$data[-datalist$folds[[fold]],])
    
    summaries[[fold]] = summary(model)%>%
      .$coefficients%>%
      data.frame()%>%
      as.tibble()%>%
      rownames_to_column(var = "Variable")%>%
      rename(P_Value = Pr...z..,
             Z_Value = z.value,
             Std_Error = Std..Error)%>%
      mutate(Significant = ifelse(P_Value < 0.05, "Significant", "Not Significant"),
             Fold = fold)
    
    predictions[[fold]] = predict(model, 
                                  newdata = datalist$data[datalist$folds[[fold]],],
                                  type = "raw")%>%
      data.frame(pred = ., 
                 obs = datalist$data[datalist$folds[[fold]],]$Class)
  }
  
  return(list(summaries = summaries, predictions = predictions))
  
}

ExtractConfusionMatrices = function(confList){
  
  confList%>%
    
}

GlmEvaluate = function(caretglm, plot=TRUE){
  
  summ = do.call(caretglm$summaries, rbind)
  
  preds = lapply(caretglm$predictions, function(x) confusionMatrix(table(x), reference = "Episode"))
}
```


```{r, echo=FALSE, include=FALSE}
for(i in 1:7){
  #--------------------------------------#
  ## GLM ##
  glmModels
  datasets[[i]]$glm$model = train(Class ~.,
                   method = "glm",
                   family=binomial,
                   data=datasets[[i]]$train)
  
  datasets[[i]]$glm$summary = summary(datasets[[i]]$glm$model$finalModel)%>%
      .$coefficients%>%
      data.frame()%>%
      as.tibble()%>%
      rownames_to_column(var = "Variable")%>%
      rename(P_Value = Pr...z..,
             Z_Value = z.value,
             Std_Error = Std..Error)%>%
      mutate(Significant = ifelse(P_Value < 0.05, "Significant", "Not Significant"),
             Section = s)
  
  datasets[[i]]$glm$predictions = predict(
      datasets[[i]]$glm$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## RanfomForest ##
  tc = trainControl("cv", 
                    number = 10, 
                    savePredictions=T, 
                    search = "random",
                    summaryFunction = twoClassSummary,
                    classProbs = T)
  
  datasets[[i]]$rf$model = train(Class~., 
                                  data=datasets[[i]]$train, 
                                  method="rf", 
                                  metric="ROC", 
                                  tuneLength=15, 
                                  trControl=tc)
  
  datasets[[i]]$rf$summary = randomForest::importance(datasets[[i]]$rf$model$finalModel)%>%
    as.data.frame()%>%
    rownames_to_column("Variable")%>%
    arrange(MeanDecreaseGini)
  
  datasets[[i]]$rf$predictions = predict(
      datasets[[i]]$rf$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## XGBoost ##
  
  tc = trainControl("cv", 10, 
                      savePredictions=T, 
                      search = "random",
                      summaryFunction = twoClassSummary,
                      classProbs = T) 
    
  datasets[[i]]$xgb$model = train(Class ~.,
               method = "xgbTree",
               data=datasets[[i]]$train,
               trControl = tc,
               metric = "ROC",
               tuneLength = 3)
  
  datasets[[i]]$xgb$predictions = predict(
      datasets[[i]]$xgb$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## SVM ##
  
  tc = trainControl("cv", 15, 
                      savePredictions=T, 
                      summaryFunction = twoClassSummary,
                      classProbs = T,
                      search = "random")
    
  datasets[[i]]$svm$model = train(Class ~.,
               method = "svmLinear",
               data=datasets[[i]]$train,
               trControl = tc,
               tuneLength = 3,
               metric = "ROC")
  
  datasets[[i]]$svm$predictions = predict(
      datasets[[i]]$svm$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## NN ##
  
  tc = trainControl(method = 'cv', number = 50, 
                    classProbs = TRUE, 
                    summaryFunction = twoClassSummary, 
                    savePredictions = TRUE,
                    search = "random")
  
  datasets[[i]]$nn$model = train(Class ~.,
                 method = "nnet",
                 data=datasets[[i]]$train,
                 trControl = tc,
                 tuneLength = 5,
                 metric = "ROC")
  
  datasets[[i]]$nn$predictions = predict(
      datasets[[i]]$nn$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
}
```

# Results

```{r}
results = data.frame()

for(i in 1:7){
  
  temp = data.frame()
  
  for(mod in 3:7){
    
    confTable = datasets[[i]][[mod]]$predictions%>%
      table()
    
    temp = confusionMatrix(confTable, positive = "Episode")%>%
      .$byClass%>%
      t()%>%
      as.tibble()%>%
      mutate(Model = names(datasets[[i]])[mod],
             OverallAcc = sum(datasets[[i]][[mod]]$predictions$pred == datasets[[i]][[mod]]$predictions$obs)/length(datasets[[i]][[mod]]$predictions$pred))%>%
      select(Model, OverallAcc, everything())%>%
      rbind(temp,.)
    
    
    
  }

  temp = temp%>%
    mutate(Section = i)%>%
    select(Section, everything())
  
  results = rbind(results, temp)
  
}
```

# Conclusion

## Overall Accuracy
*NB: Section 7 refers to HF_4 and LF_3 *

Here's the full table of results for browsing:

```{r}
results
```

Overall, the most accurate model is:

```{r}
results%>%
  arrange(-OverallAcc)%>%
  slice(1)

datasets[[7]][["xgb"]]$predictions%>%
  table()
```

## Balanced Accuracy
For balanced accuracy:

```{r}
results%>%
  arrange(-`Balanced Accuracy`)%>%
  slice(1)

datasets[[3]][["nn"]]$predictions%>%table()
```

##Sensitivity
For sensitivity:
```{r}
results%>%
  arrange(-Sensitivity)%>%
  slice(1)

datasets[[2]][["nn"]]$predictions%>%table()
```

## Specificity
For Specificity
```{r}
results%>%
  arrange(-Specificity)%>%
  slice(1)

datasets[[1]][["xgb"]]$predictions%>%table()
```

## Plot

```{r}
results%>%
  gather("Metric", "value", 3:14)%>%
  ggplot(aes(y=Metric,x=Section))+
  geom_tile(aes(fill=value))+
  facet_wrap(Model~.)+
  scale_fill_viridis()+
  theme_minimal()+
  labs(title = "Final Model Prediction Metrics",
       caption = "Section: Portion of 30 minute window\n\"Section 7\" refers to HF_4 and LF_3")+
  scale_x_continuous(breaks = 1:7, labels = 1:7)
```

