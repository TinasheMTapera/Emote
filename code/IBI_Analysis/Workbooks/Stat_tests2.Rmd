---
title: "ML & Hypothesis Tests"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---
```{r, include=FALSE}
#packages
library(tidyverse)
library(caret)
library(DAAG)
library(viridis)
library(DMwR)
library(ROSE)
```


Here is the completed data set:
```{r}
mydat = read.csv("~/Desktop/data_out.csv")%>%
  filter(threshold == 250 & winds == 2)%>%
  select(-winds, - threshold, -index)

mydat%>%
  filter(complete.cases(.))%>%
  summary()
```

# What is the distribution of stress for the usable surveys?
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  select(Y, Stress)%>%
  ggplot(aes(x=Y, y=Stress))+
  geom_boxplot(aes(fill = Y))+
  theme_minimal()+
  labs(title = "Distribution of Stress by Condition")
```

Episodes may have higher stress levels than controls. We will remove the stressed out outliers for analysis. We will also do within participant z-score centering, like so:

```{r}
#variable distribution without z-score scaling
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>%
  select(ID, Y:HRVi)%>%
  # group_by(ID)%>%
  # mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>%
  # ungroup()%>%
  filter(complete.cases(.))%>%
  gather("variable", "value", SDNN:HRVi)%>%
  ggplot(aes(value, color = variable))+
  geom_density(alpha = 0.5)+
  theme_minimal()+
  scale_color_viridis(discrete = TRUE)+
  labs(title = "Density Plots of Raw Variables")

#variable distribution with z-score scaling
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform within-subject
  ungroup()%>%
  filter(complete.cases(.))%>%
  gather("variable", "value", SDNN:HRVi)%>%
  ggplot(aes(value, color = variable))+
  geom_density(alpha = 0.5)+
  theme_minimal()+
  scale_color_viridis(discrete = TRUE)+
  labs(title = "Density Plots of Within-Participant Z Score Variables")
```


# Do features differ statistically by group?

What are the mean values for each of the variables by condition?
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  group_by(Y)%>%
  summarise_at(.vars = vars(SDNN:HRVi), funs(mean(.)))
```

Note that we now have 126 usable cases as the Z-scoring process removed participants with 1 survey or with uniform values for all their surveys.

```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  gather("Variable", "Value", 2:11)%>%
  ggplot(aes(x=Variable, y=Value))+
  geom_boxplot(aes(fill=Y))+
  theme_minimal()+
  labs(y = "Z Score", title = "Boxplot of All Variables By Condition")
```

Are there differences using a regular T-Test? The table below shows $p$-values at each variable's comparison:
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  summarise_at(funs(t.test(x = .[Y == "Control"],
                           y = .[Y != "Control"])$p.value),
               .vars = vars(SDNN:HRVi))
```

Another approach using instead, the permuation T-test, gives us $p$-values too:
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  summarise_at(funs(twotPermutation(.[Y == "Control"],
                                    .[Y != "Control"], 
                                    plotit = F)),
               .vars = vars(SDNN:HRVi))
```

It seems like none of the variables are statistically different.

# Predicting Condition

We use simple over- and under-sampling as well as ROSE and SMOTE to deal with the class imbalance problem.
```{r}
# Here we set the train and test set

set.seed(2)
mydat2 = mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  rename(Class = Y)

train = createDataPartition(y = mydat2$Class, p = 0.7, list = FALSE)

# next, we resample from the training set with different methods
down = downSample(select(mydat2, -Class)[train,], mydat2$Class[train])
up = upSample(select(mydat2, -Class)[train,], mydat2$Class[train])
smote = SMOTE(Class ~ ., data  = as.data.frame(mydat2[train,]))
rose = ROSE(Class ~ ., data  = mydat2[train,])$data

#some indices for loops
sample_Names = c("None", "DownSample", "UpSample", "SMOTE", "ROSE")
samples = list(mydat2[train,], down, up, smote, rose)
names(samples) = sample_Names
```

## Logistic Regression/GLM
Fitting a glm and inspecting the model fit:
```{r, message=FALSE, warning=FALSE}
#empty vectors for sampling methods
glmFitSummaries = vector("list", length(sample_Names))
glmModels = vector("list", length(sample_Names))

#loop through the different samples and fit the model for each
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  #tc = trainControl("cv", 5, savePredictions=T)
  glmModels[[names(samples)[s]]] = train(Class ~.,
                 method = "glm",
                 family=binomial,
                 data=samples[[s]]) #,
                 #trControl = tc)
  
  #collect variable significance metrics
  glmFitSummaries[[names(samples)[s]]] = summary(glmFitSummaries$finalModel)$coefficients%>%
    data.frame()%>%
    as.tibble()%>%
    rownames_to_column(var = "Variable")%>%
    rename(P_Value = Pr...z..,
           Z_Value = z.value,
           Std_Error = Std..Error)%>%
    mutate(Significant = ifelse(P_Value < 0.05, "Significant", "Not Significant"),
           Sampling = names(samples)[[s]])
}

#print the GLM variable estimates and significance for each model
do.call(rbind, glmFitSummaries)%>%
  select(Sampling, Variable, Significant, everything())
```

Evaluating prediction:
```{r}
#empy vectors for results
glmResults = vector("list", length(sample_Names))
glmPredictions = vector("list", length(sample_Names))

#loop through the different samples and predict test cases
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  pred = predict(
    glmModels[[names(samples)[s]]], 
    newdata = mydat2[-train,],
    type = "raw")%>%
    data.frame(pred = ., obs = mydat2$Class[-train])
  
  glmPredictions[[names(samples)[s]]] = pred
  
  glmResults[[names(samples)[s]]] = confusionMatrix(pred$pred, 
                                                    pred$obs, 
                                                    positive = "Episode")
}

#data frame for compiling results
comparisons = data.frame()

#loop through each sampling method's results and compile
for(x in 1:length(glmResults)){
  
  comparisons = glmResults[[names(samples)[x]]]$byClass%>%
    data.frame(Value = .)%>%
    rownames_to_column()%>%
    rename(Variable = rowname)%>%
    mutate(Sampling = names(samples)[x])%>%
    select(Sampling, everything())%>%
    rbind(comparisons, .)
}

#plot
comparisons%>%
  ggplot(aes(x = Variable, y = Value))+
  geom_point(aes(color = Sampling), alpha = 0.4, size = 5)+
  theme_minimal()+
  labs(title = "GLM: Classification Metrics by Sampling Method")+
  coord_flip()+
  theme(text = element_text(size = 15))+
  scale_color_viridis(discrete = TRUE)
```

The best balanced accuracy for GLM is with no balancing, 

## RandomForest
Fitting a random forest model for each resampling method:
```{r}
#empty vectors for sampling methods
rfFitSummaries = vector("list", length(sample_Names))
rfModels = vector("list", length(sample_Names))

#loop through the different samples and fit the model for each
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  tc = trainControl("cv", 10, savePredictions=T, search = "grid")
  
  rfModels[[names(samples)[s]]] = train(Class ~.,
               method = "rf",
               data=samples[[s]],
               trControl = tc,
               metric = "Kappa") #use kappa as metric for imbalance
  
  #collect variable IMPORTANCE metrics smaller decreaseGini is better
  rfFitSummaries[[names(samples)[s]]] = rfModels[[names(samples)[s]]]$finalModel$importance%>%
    data.frame()%>%
    as.tibble()%>%
    rownames_to_column(var = "Variable")%>%
    mutate(Sampling = names(samples)[[s]])%>%
    select(Sampling, everything())
}

# plotting solution found here: 
# https://github.com/dgrtwo/drlib/blob/master/R/reorder_within.R
# Much appreciated @drlib

reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
  new_x <- paste(x, within, sep = sep)
  stats::reorder(new_x, by, FUN = fun)
}
scale_x_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}
scale_y_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, "", x), ...)
}

do.call(rbind, rfFitSummaries)%>%
  mutate_if(is.character, as.factor)%>%
  ggplot(., aes(x = reorder_within(Variable, MeanDecreaseGini, Sampling),
                y = MeanDecreaseGini,
                fill = Variable))+
  geom_bar(stat = "identity", show.legend = FALSE)+
  scale_x_reordered()+
  facet_wrap(~Sampling, scales = "free")+
  coord_flip()+
  labs(x="Variable",
       title = "Variable Importance Per Sampling Method",
       subtitle = "(Lower Values Indicate More Importance in Making Stronger Final Predictions)")+
  theme_minimal()+
  scale_fill_viridis(discrete=TRUE)
```
In this modeling case the most important variable for prediction varies with the resampling approach, so it's hard to tell what's being said here, but pNN50 sticks out in 3 of the 5 models.

Evaluating prediction:
```{r}
#empy vectors for results
rfResults = vector("list", length(sample_Names))
rfPredictions = vector("list", length(sample_Names))

#loop through the different samples and predict test cases
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  rfPredictions[[names(samples)[s]]] = predict(rfModels[[names(samples)[s]]],newdata=mydat2[-train,])%>%
  tibble(pred = ., obs = mydat2[-train,]$Class)
  
  rfResults[[names(samples)[s]]] = confusionMatrix(pred$pred, 
                                                    pred$obs, 
                                                    positive = "Episode")
}

#data frame for compiling results
comparisons = data.frame()

#loop through each sampling method's results and compile
for(x in 1:length(glmResults)){
  
  comparisons = rfResults[[names(samples)[x]]]$byClass%>%
    data.frame(Value = .)%>%
    rownames_to_column()%>%
    rename(Variable = rowname)%>%
    mutate(Sampling = names(samples)[x])%>%
    select(Sampling, everything())%>%
    rbind(comparisons, .)
}

#plot
comparisons%>%
  ggplot(aes(x = Variable, y = Value))+
  geom_point(aes(color = Sampling), alpha = 0.4, size = 5)+
  theme_minimal()+
  labs(title = "Random Forest: Classification Metrics by Sampling Method")+
  coord_flip()+
  theme(text = element_text(size = 15))+
  scale_color_viridis(discrete = TRUE)
```

## SVM
Fitting and inspecting the model:
```{r}
#empty vectors for sampling methods
svmFitSummaries = vector("list", length(sample_Names))
svmModels = vector("list", length(sample_Names))

#loop through the different samples and fit the model for each
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  tc = trainControl("cv", 10, 
                    savePredictions=T, 
                    summaryFunction = twoClassSummary,
                    search = "random",
                    classProbs = TRUE)
  
  svmModels[[names(samples)[s]]] = train(Class ~.,
               method = "svmLinear2",
               data=samples[[s]],
               trControl = tc,
               tuneLength = 30,
               metric = "ROC") #use ROC
  
  #collect summaries
  svmFitSummaries[[names(samples)[s]]] = svmModels[[names(samples)[s]]]$finalModel%>%summary()
}

svmFitSummaries
```

Evaluating prediction:
```{r}
#empy vectors for results
svmResults = vector("list", length(sample_Names))
svmPredictions = vector("list", length(sample_Names))

#loop through the different samples and predict test cases
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  svmPredictions[[names(samples)[s]]] = predict(svmModels[[names(samples)[s]]],newdata=mydat2[-train,])%>%
  tibble(pred = ., obs = mydat2[-train,]$Class)
  
  svmResults[[names(samples)[s]]] = confusionMatrix(pred$pred, 
                                                    pred$obs, 
                                                    positive = "Episode")
}

#data frame for compiling results
comparisons = data.frame()

#loop through each sampling method's results and compile
for(x in 1:length(svmResults)){
  
  comparisons = svmResults[[names(samples)[x]]]$byClass%>%
    data.frame(Value = .)%>%
    rownames_to_column()%>%
    rename(Variable = rowname)%>%
    mutate(Sampling = names(samples)[x])%>%
    select(Sampling, everything())%>%
    rbind(comparisons, .)
}

#plot
comparisons%>%
  ggplot(aes(x = Variable, y = Value))+
  geom_point(aes(color = Sampling), alpha = 0.4, size = 5)+
  theme_minimal()+
  labs(title = "SVM: Classification Metrics by Sampling Method")+
  coord_flip()+
  theme(text = element_text(size = 15))+
  scale_color_viridis(discrete = TRUE)
```
SVM doesn't respond to differences in sampling methods in this case.

## NeuralNet
Fitting and inspecing the model:
```{r, include=FALSE}
tc = trainControl(method = 'cv', number = 10, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = TRUE)
netFit = train(Class ~.,
               method = "nnet",
               data=mydat2[train,],
               trControl = tc,
               tuneGrid=expand.grid(size=c(10), decay=c(0.1,0.01)))
```

```{r}
netFit
```

Evaluating prediction:
```{r}
pred = predict(netFit,newdata = mydat2[-train,])%>%
  tibble(pred = ., obs = mydat2[-train,]$Y)

confusionMatrix(pred$pred, pred$obs, positive = "Episode")
```

# Conclusion

* There were no variables with statistically significant differences between conditions
* The machine learning algorithms attempt to predict every case to 