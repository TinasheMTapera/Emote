---
title: "ML & Hypothesis Tests"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---
```{r, include=FALSE}
#packages
library(tidyverse)
library(caret)
library(DAAG)
library(viridis)
library(DMwR)
library(ROSE)
```


Here is the completed data set:
```{r}
mydat = read.csv("~/Desktop/data_out.csv")%>%
  filter(threshold == 250 & winds == 2)%>%
  select(-winds, - threshold, -index)

mydat%>%
  filter(complete.cases(.))%>%
  summary()
```

# What is the distribution of stress for the usable surveys?
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  select(Y, Stress)%>%
  ggplot(aes(x=Y, y=Stress))+
  geom_boxplot(aes(fill = Y))+
  theme_minimal()+
  labs(title = "Distribution of Stress by Condition")
```

Episodes may have higher stress levels than controls. We will remove the stressed out outliers for analysis. We will also do within participant z-score centering, like so:

```{r}
#variable distribution without z-score scaling
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>%
  select(ID, Y:HRVi)%>%
  # group_by(ID)%>%
  # mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>%
  # ungroup()%>%
  filter(complete.cases(.))%>%
  gather("variable", "value", SDNN:HRVi)%>%
  ggplot(aes(value, color = variable))+
  geom_density(alpha = 0.5)+
  theme_minimal()+
  scale_color_viridis(discrete = TRUE)+
  labs(title = "Density Plots of Raw Variables")

#variable distribution with z-score scaling
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform within-subject
  ungroup()%>%
  filter(complete.cases(.))%>%
  gather("variable", "value", SDNN:HRVi)%>%
  ggplot(aes(value, color = variable))+
  geom_density(alpha = 0.5)+
  theme_minimal()+
  scale_color_viridis(discrete = TRUE)+
  labs(title = "Density Plots of Within-Participant Z Score Variables")
```


# Do features differ statistically by group?

What are the mean values for each of the variables by condition?
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  group_by(Y)%>%
  summarise_at(.vars = vars(SDNN:HRVi), funs(mean(.)))
```

Note that we now have 126 usable cases as the Z-scoring process removed participants with 1 survey or with uniform values for all their surveys.

```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  gather("Variable", "Value", 2:11)%>%
  ggplot(aes(x=Variable, y=Value))+
  geom_boxplot(aes(fill=Y))+
  theme_minimal()+
  labs(y = "Z Score", title = "Boxplot of All Variables By Condition")
```

Are there differences using a regular T-Test? The table below shows $p$-values at each variable's comparison:
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  summarise_at(funs(t.test(x = .[Y == "Control"],
                           y = .[Y != "Control"])$p.value),
               .vars = vars(SDNN:HRVi))
```

Another approach using instead, the permuation T-test, gives us $p$-values too:
```{r}
mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  summarise_at(funs(twotPermutation(.[Y == "Control"],
                                    .[Y != "Control"], 
                                    plotit = F)),
               .vars = vars(SDNN:HRVi))
```

It seems like none of the variables are statistically different.

# Predicting Condition

We use simple over- and under-sampling as well as ROSE and SMOTE to deal with the class imbalance problem.
```{r}
# Here we set the train and test set

set.seed(2)
mydat2 = mydat%>%
  filter(complete.cases(.))%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(ID, Y:HRVi)%>%
  group_by(ID)%>%
  mutate_at(.vars = vars(SDNN:HRVi), funs(scale))%>% #z score transform
  ungroup()%>%
  filter(complete.cases(.))%>%
  select(Y:HRVi)%>%
  rename(Class = Y)

train = createDataPartition(y = mydat2$Class, p = 0.7, list = FALSE)

# next, we resample from the training set with different methods
down = downSample(select(mydat2, -Class)[train,], mydat2$Class[train])
up = upSample(select(mydat2, -Class)[train,], mydat2$Class[train])
smote = SMOTE(Class ~ ., data  = as.data.frame(mydat2[train,]))
rose = ROSE(Class ~ ., data  = mydat2[train,])$data

#some indices for loops
sample_Names = c("None", "DownSample", "UpSample", "SMOTE", "ROSE")
samples = list(mydat2[train,], down, up, smote, rose)
names(samples) = sample_Names
```

## Logistic Regression/GLM
Fitting a glm and inspecting the model fit:
```{r, message=FALSE, warning=FALSE}
#empty vectors for sampling methods
glmFitSummaries = vector("list")
glmModels = vector("list")

#loop through the different samples and fit the model for each
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  glmModels[[names(samples)[s]]] = train(Class ~.,
                 method = "glm",
                 family=binomial,
                 data=samples[[s]])
  
  #collect variable significance metrics
  glmFitSummaries[[names(samples)[s]]] = summary(glmModels[[names(samples)[s]]]$finalModel)%>%
    .$coefficients%>%
    data.frame()%>%
    as.tibble()%>%
    rownames_to_column(var = "Variable")%>%
    rename(P_Value = Pr...z..,
           Z_Value = z.value,
           Std_Error = Std..Error)%>%
    mutate(Significant = ifelse(P_Value < 0.05, "Significant", "Not Significant"),
           Sampling = names(samples)[[s]])
}

```
```{r}
#print the GLM variable estimates and significance for each model
do.call(rbind, glmFitSummaries)%>%
  select(Sampling, Variable, Significant, everything())
```
SMOTE resampling produced the highest number of significant predictors.

Evaluating prediction:
```{r}
#empy vectors for results
glmResults = vector("list")
glmPredictions = vector("list")

#loop through the different samples and predict test cases
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  pred = predict(
    glmModels[[names(samples)[s]]], 
    newdata = mydat2[-train,],
    type = "raw")%>%
    data.frame(pred = ., obs = mydat2$Class[-train])
  
  glmPredictions[[names(samples)[s]]] = pred
  
  glmResults[[names(samples)[s]]] = confusionMatrix(pred$pred, 
                                                    pred$obs, 
                                                    positive = "Episode")
}

#data frame for compiling results
comparisons = data.frame()

#loop through each sampling method's results and compile
for(x in 1:length(glmResults)){
  
  comparisons = glmResults[[names(samples)[x]]]$byClass%>%
    data.frame(Value = .)%>%
    rownames_to_column()%>%
    rename(Variable = rowname)%>%
    mutate(Sampling = names(samples)[x])%>%
    select(Sampling, everything())%>%
    rbind(comparisons, .)
}

#plot
comparisons%>%
  ggplot(aes(x = Variable, y = Value))+
  geom_point(aes(color = Sampling, shape = Sampling), alpha = 0.7, size = 5)+
  theme_minimal()+
  labs(title = "GLM: Classification Metrics by Sampling Method", x = "Metric")+
  coord_flip()+
  theme(text = element_text(size = 15))+
  scale_color_viridis(discrete = TRUE)
```

Likewise, the SMOTE resampling produced the highest accuracy, precision, and sensitivity.

##Results
```{r}
lapply(glmPredictions, table)
do.call(rbind, glmPredictions)%>%
  rownames_to_column()%>%
  rename(S = rowname)%>%
  separate(S, c("Sampling", "xx"), sep = "\\.")%>%
  select(-xx)%>%
  group_by(Sampling)%>%
  summarise(Overall_Accuracy = sum(pred == obs)/n())%>%
  arrange(desc(Overall_Accuracy))
final_glm = comparisons%>%
  filter(Sampling == "SMOTE")%>%
  arrange(Variable)%>%
  mutate(Model = "LogisticRegression")%>%
  select(Model, everything())

final_glm
```


## RandomForest
Fitting a random forest model for each resampling method:
```{r, echo=FALSE}
#empty vectors for sampling methods
rfFitSummaries = vector("list")
rfModels = vector("list")

#loop through the different samples and fit the model for each
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  tc = trainControl("cv", 10, 
                    savePredictions=T, 
                    search = "random",
                    summaryFunction = prSummary) #use precision for imbalance
  
  rfModels[[names(samples)[s]]] = train(Class ~.,
               method = "rf",
               data=samples[[s]],
               trControl = tc,
               metric = "Precision",
               tuneLength = 20) #use precision for imbalance
  
  #collect variable IMPORTANCE metrics smaller decreaseGini is better
  rfFitSummaries[[names(samples)[s]]] = rfModels[[names(samples)[s]]]$finalModel$importance%>%
    data.frame()%>%
    as.tibble()%>%
    rownames_to_column(var = "Variable")%>%
    mutate(Sampling = names(samples)[[s]])%>%
    select(Sampling, everything())
}

```
```{r}
# plotting solution found here: 
# https://github.com/dgrtwo/drlib/blob/master/R/reorder_within.R
# Much appreciated @drlib

reorder_within <- function(x, by, within, fun = mean, sep = "___", ...) {
  new_x <- paste(x, within, sep = sep)
  stats::reorder(new_x, by, FUN = fun)
}
scale_x_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_x_discrete(labels = function(x) gsub(reg, "", x), ...)
}
scale_y_reordered <- function(..., sep = "___") {
  reg <- paste0(sep, ".+$")
  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, "", x), ...)
}

do.call(rbind, rfFitSummaries)%>%
  mutate_if(is.character, as.factor)%>%
  ggplot(., aes(x = reorder_within(Variable, MeanDecreaseGini, Sampling),
                y = MeanDecreaseGini,
                fill = Variable))+
  geom_bar(stat = "identity", show.legend = FALSE)+
  scale_x_reordered()+
  facet_wrap(~Sampling, scales = "free")+
  coord_flip()+
  labs(x="Variable",
       title = "Variable Importance Per Sampling Method",
       subtitle = "(Lower Values Indicate More Importance in Making Stronger Final Predictions)")+
  theme_minimal()+
  scale_fill_viridis(discrete=TRUE)
```

Using random forest the most important variable for prediction varies with the resampling approach, so it's hard to tell what's being said here.

Evaluating prediction:
```{r}
#empy vectors for results
rfResults = vector("list")
rfPredictions = vector("list")

#loop through the different samples and predict test cases
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  rfPredictions[[names(samples)[s]]] = predict(rfModels[[names(samples)[s]]],newdata=mydat2[-train,])%>%
  tibble(pred = ., obs = mydat2[-train,]$Class)
  
  rfResults[[names(samples)[s]]] = confusionMatrix(pred$pred, 
                                                    pred$obs, 
                                                    positive = "Episode")
}

#data frame for compiling results
comparisons = data.frame()

#loop through each sampling method's results and compile
for(x in 1:length(glmResults)){
  
  comparisons = rfResults[[names(samples)[x]]]$byClass%>%
    data.frame(Value = .)%>%
    rownames_to_column()%>%
    rename(Variable = rowname)%>%
    mutate(Sampling = names(samples)[x])%>%
    select(Sampling, everything())%>%
    rbind(comparisons, .)
}

#plot
comparisons%>%
  ggplot(aes(x = Variable, y = Value))+
  geom_point(aes(color = Sampling, shape = Sampling), alpha = 0.7, size = 5)+
  theme_minimal()+
  labs(title = "Random Forest: Classification Metrics by Sampling Method",
       x = "Metric")+
  coord_flip()+
  theme(text = element_text(size = 15))+
  scale_color_viridis(discrete = TRUE)
```

Sampling approach has no effect on the model accuracies with random forest.

##Results
```{r}
lapply(rfPredictions, table)
do.call(rbind, rfPredictions)%>%
  rownames_to_column()%>%
  rename(S = rowname)%>%
  separate(S, c("Sampling", "xx"), sep = "\\.")%>%
  select(-xx)%>%
  group_by(Sampling)%>%
  summarise(Overall_Accuracy = sum(pred == obs)/n())%>%
  arrange(desc(Overall_Accuracy))
final_RF
```


## Support Vector Machine
Fitting and inspecting the model:
```{r, echo=FALSE, include=FALSE}
#empty vectors for sampling methods
svmFitSummaries = vector("list")
svmModels = vector("list")

#loop through the different samples and fit the model for each
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  tc = trainControl("cv", 10, 
                    savePredictions=T, 
                    summaryFunction = prSummary, #use precision for imbalance
                    classProbs = T,
                    search = "random")
  
  svmModels[[names(samples)[s]]] = train(Class ~.,
               method = "svmLinear2",
               data=samples[[s]],
               trControl = tc,
               tuneLength = 20,
               metric = "Precision") #use precision for imbalance
  
  #collect summaries
  svmFitSummaries[[names(samples)[s]]] = svmModels[[names(samples)[s]]]$finalModel%>%summary()
}
```
```{r}
svmFitSummaries
```
Not much to interpret from the summaries.

Evaluating prediction:
```{r}
#empy vectors for results
svmResults = vector("list")
svmPredictions = vector("list")

#loop through the different samples and predict test cases
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  svmPredictions[[names(samples)[s]]] = predict(svmModels[[names(samples)[s]]],newdata=mydat2[-train,])%>%
  tibble(pred = ., obs = mydat2[-train,]$Class)
  
  svmResults[[names(samples)[s]]] = confusionMatrix(pred$pred, 
                                                    pred$obs, 
                                                    positive = "Episode")
}

#data frame for compiling results
comparisons = data.frame()

#loop through each sampling method's results and compile
for(x in 1:length(svmResults)){
  
  comparisons = svmResults[[names(samples)[x]]]$byClass%>%
    data.frame(Value = .)%>%
    rownames_to_column()%>%
    rename(Variable = rowname)%>%
    mutate(Sampling = names(samples)[x])%>%
    select(Sampling, everything())%>%
    rbind(comparisons, .)
}

#plot
comparisons%>%
  ggplot(aes(x = Variable, y = Value))+
  geom_point(aes(color = Sampling, shape = Sampling), alpha = 0.7, size = 5)+
  theme_minimal()+
  labs(title = "SVM: Classification Metrics by Sampling Method",
       x = "Metric")+
  coord_flip()+
  theme(text = element_text(size = 15))+
  scale_color_viridis(discrete = TRUE)
```
SVM doesn't respond to differences in sampling methods in this case.

##Results
```{r}
lapply(svmPredictions, table)
do.call(rbind, svmPredictions)%>%
  rownames_to_column()%>%
  rename(S = rowname)%>%
  separate(S, c("Sampling", "xx"), sep = "\\.")%>%
  select(-xx)%>%
  group_by(Sampling)%>%
  summarise(Overall_Accuracy = sum(pred == obs)/n())%>%
  arrange(desc(Overall_Accuracy))

final_svm = comparisons%>%
  filter(Sampling == "None")%>%
  arrange(Variable)%>%
  mutate(Model = "SVM")%>%
  select(Model, everything())

final_svm
```


## NeuralNet
Fitting and inspecing the model:
```{r, include=FALSE}
#empty vectors for sampling methods
nnetFitSummaries = vector("list")
nnetModels = vector("list")

#loop through the different samples and fit the model for each
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])

  tc = trainControl(method = 'cv', number = 10, 
                    classProbs = TRUE, 
                    summaryFunction = prSummary, 
                    savePredictions = TRUE,
                    search = "random")
  
  #fit model
  nnetModels[[names(samples)[s]]] = train(Class ~.,
                 method = "nnet",
                 data=mydat2[train,],
                 trControl = tc,
                 metric = "Precision")
  
  #collect summaries
  nnetFitSummaries[[names(samples)[s]]] = nnetModels[[names(samples)[s]]]$finalModel
}

```

```{r}
nnetFitSummaries
```

Evaluating prediction:
```{r}
#empy vectors for results
nnetResults = vector("list")
nnetPredictions = vector("list")

#loop through the different samples and predict test cases
#store results in empty vectors
for(s in 1:length(samples)){
  print(names(samples)[[s]])
  
  nnetPredictions[[names(samples)[s]]] = predict(nnetModels[[names(samples)[s]]],newdata=mydat2[-train,])%>%
  tibble(pred = ., obs = mydat2[-train,]$Class)
  
  nnetResults[[names(samples)[s]]] = confusionMatrix(pred$pred, 
                                                    pred$obs, 
                                                    positive = "Episode")
}

#data frame for compiling results
comparisons = data.frame()

#loop through each sampling method's results and compile
for(x in 1:length(nnetResults)){
  
  comparisons = nnetResults[[names(samples)[x]]]$byClass%>%
    data.frame(Value = .)%>%
    rownames_to_column()%>%
    rename(Variable = rowname)%>%
    mutate(Sampling = names(samples)[x])%>%
    select(Sampling, everything())%>%
    rbind(comparisons, .)
}

#plot
comparisons%>%
  ggplot(aes(x = Variable, y = Value))+
  geom_point(aes(color = Sampling, shape = Sampling), alpha = 0.7, size = 5)+
  theme_minimal()+
  labs(title = "Neural Network: Classification Metrics by Sampling Method",
       x = "Metric")+
  coord_flip()+
  theme(text = element_text(size = 15))+
  scale_color_viridis(discrete = TRUE)
```
Neural Net is also unresponsive to the sampling approach.

##Results
```{r}
lapply(nnetPredictions, table)
do.call(rbind, nnetPredictions)%>%
  rownames_to_column()%>%
  rename(S = rowname)%>%
  separate(S, c("Sampling", "xx"), sep = "\\.")%>%
  select(-xx)%>%
  group_by(Sampling)%>%
  summarise(Overall_Accuracy = sum(pred == obs)/n())%>%
  arrange(desc(Overall_Accuracy))

final_nn = comparisons%>%
  filter(Sampling == "None")%>%
  arrange(Variable)%>%
  mutate(Model = "NeuralNetwork")%>%
  select(Model, everything())

final_nn
```

# Conclusion
This table ranks all of the best models for each metric:
```{r}
rbind(final_glm, final_RF, final_svm, final_nn)%>%
  group_by(Variable)%>%
  arrange(Variable, desc(Value))
```

* The best model is the Logistic Regression for balanced accuracy
* For precision the best model is Logistic Regression
* For sensitivity the best model is Logistic regression
* For specificity logistic regression underperformed
* The machine learning algorithms attempt to predict every case to "control" even after trying to adjust for resampling methods.

The overall accuracy is 83% for logistic regression, 81% for random forest, 86% for the SVM, and 86% for neural network