---
title: "Modelling Frequency2"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    theme: united
  number_sections: true
---

```{r, include=FALSE}
library(tidyverse, quietly = T)
library(caret, quietly = T)
library(DMwR, quietly = T)
library(ROSE, quietly = T)
library(viridis, quietly = T)
library(DAAG, quietly = T)
```

# Predicting Condition

We use downsampling to balance the cases. We set up a number of different models:

* There are 7 data subsets, 1 for each frequency domain section and 1 extra using the frequency domain portion with the largest portion of data

* For each data subset, we predict condition using Logistic regression, XGBoost, RandomForest, SVM, and a basic Neural Network.

## Which Variables Have Enough Data?

```{r, include=FALSE}
#save participants who qualify with both episode and control
pp = c(201, 202, 207, 214, 215, 217, 218, 219, 226, 228)
```

```{r}
#how much data is available in each section?

features_output%>%
  filter(ID %in% pp)%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(Avg_niHR:LFHF_6)%>%
  summarise_at(.vars = vars(Avg_niHR:LFHF_6), funs(sum(!is.na(.))))%>%
  gather("variable", "Rows")%>%
  arrange(-Rows)
```

```{r}
features_output%>%
  filter(ID %in% pp)%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(Avg_niHR:LFHF_6)%>%
  Amelia::missmap()
```

# Machine Learning

## Setup
```{r}
# Here we set the train and test set
set.seed(2)
mydat = features_output%>%
  filter(ID %in% pp)%>%
  filter(!(Stress > 4 & Y == "Control"))%>% #filter high stress
  select(Y, Avg_niHR:LFHF_6)%>%
  rename(Class = Y)

#set up a list of unnamed training sets
# loop through list and select variables dynamically
# partition at 70%
# downsample training set

datasets = list()

for(n in 1:6){
  
  temp_dat = mydat%>%
    select(Class, Avg_niHR:Avg_HR, contains(as.character(n)))%>%
    filter(complete.cases(.))
  
  ind = createDataPartition(y = temp_dat$Class, p = 0.7, list = FALSE)
  
  datasets[[n]] = list(train = downSample(select(temp_dat, -Class)[ind,], temp_dat$Class[ind]),
                       test = temp_dat[-ind,])
    
}

#statistically significant diff variables
temp_dat = mydat%>%
  select(Class, Avg_niHR, End_niHR, Avg_HR, HF_4, LF_3)%>%
  filter(complete.cases(.))

ind = ind = createDataPartition(y = temp_dat$Class, p = 0.7, list = FALSE)

datasets[[7]] = list(train = downSample(select(temp_dat, -Class)[ind,], temp_dat$Class[ind]),
                     test = temp_dat[-ind,])
```

```{r, echo=FALSE, include=FALSE}
for(i in 1:7){
  #--------------------------------------#
  ## GLM ##
  datasets[[i]]$glm$model = train(Class ~.,
                   method = "glm",
                   family=binomial,
                   data=datasets[[i]]$train)
  
  datasets[[i]]$glm$summary = summary(datasets[[i]]$glm$model$finalModel)%>%
      .$coefficients%>%
      data.frame()%>%
      as.tibble()%>%
      rownames_to_column(var = "Variable")%>%
      rename(P_Value = Pr...z..,
             Z_Value = z.value,
             Std_Error = Std..Error)%>%
      mutate(Significant = ifelse(P_Value < 0.05, "Significant", "Not Significant"),
             Section = s)
  
  datasets[[i]]$glm$predictions = predict(
      datasets[[i]]$glm$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## RanfomForest ##
  tc = trainControl("cv", 
                    number = 10, 
                    savePredictions=T, 
                    search = "random",
                    summaryFunction = twoClassSummary,
                    classProbs = T)
  
  datasets[[i]]$rf$model = train(Class~., 
                                  data=datasets[[i]]$train, 
                                  method="rf", 
                                  metric="ROC", 
                                  tuneLength=15, 
                                  trControl=tc)
  
  datasets[[i]]$rf$summary = randomForest::importance(datasets[[i]]$rf$model$finalModel)%>%
    as.data.frame()%>%
    rownames_to_column("Variable")%>%
    arrange(MeanDecreaseGini)
  
  datasets[[i]]$rf$predictions = predict(
      datasets[[i]]$rf$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## XGBoost ##
  
  tc = trainControl("cv", 10, 
                      savePredictions=T, 
                      search = "random",
                      summaryFunction = twoClassSummary,
                      classProbs = T) 
    
  datasets[[i]]$xgb$model = train(Class ~.,
               method = "xgbTree",
               data=datasets[[i]]$train,
               trControl = tc,
               metric = "ROC",
               tuneLength = 3)
  
  datasets[[i]]$xgb$predictions = predict(
      datasets[[i]]$xgb$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## SVM ##
  
  tc = trainControl("cv", 15, 
                      savePredictions=T, 
                      summaryFunction = twoClassSummary,
                      classProbs = T,
                      search = "random")
    
  datasets[[i]]$svm$model = train(Class ~.,
               method = "svmLinear",
               data=datasets[[i]]$train,
               trControl = tc,
               tuneLength = 3,
               metric = "ROC")
  
  datasets[[i]]$svm$predictions = predict(
      datasets[[i]]$svm$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
  
  #-------------------------------------#
  ## NN ##
  
  tc = trainControl(method = 'cv', number = 50, 
                    classProbs = TRUE, 
                    summaryFunction = twoClassSummary, 
                    savePredictions = TRUE,
                    search = "random")
  
  datasets[[i]]$nn$model = train(Class ~.,
                 method = "nnet",
                 data=datasets[[i]]$train,
                 trControl = tc,
                 tuneLength = 5,
                 metric = "ROC")
  
  datasets[[i]]$nn$predictions = predict(
      datasets[[i]]$nn$model, 
      newdata = datasets[[i]]$test,
      type = "raw")%>%
      data.frame(pred = ., obs = datasets[[i]]$test$Class)
}
```

# Results

```{r}
results = data.frame()

for(i in 1:7){
  
  temp = data.frame()
  
  for(mod in 3:7){
    
    confTable = datasets[[i]][[mod]]$predictions%>%
      table()
    
    temp = confusionMatrix(confTable, positive = "Episode")%>%
      .$byClass%>%
      t()%>%
      as.tibble()%>%
      mutate(Model = names(datasets[[i]])[mod],
             OverallAcc = sum(datasets[[i]][[mod]]$predictions$pred == datasets[[i]][[mod]]$predictions$obs)/length(datasets[[i]][[mod]]$predictions$pred))%>%
      select(Model, OverallAcc, everything())%>%
      rbind(temp,.)
    
    
    
  }

  temp = temp%>%
    mutate(Section = i)%>%
    select(Section, everything())
  
  results = rbind(results, temp)
  
}
```

# Conclusion

## Overall Accuracy
*NB: Section 7 refers to HF_4 and LF_3 *

Here's the full table of results for browsing:

```{r}
results
```

Overall, the most accurate model is:

```{r}
results%>%
  arrange(-OverallAcc)%>%
  slice(1)

datasets[[5]][["xgb"]]$predictions%>%
  table()
```

Simply due to the fact that it predicted all cases to controls.

## Balanced Accuracy
For balanced accuracy:

```{r}
results%>%
  arrange(-`Balanced Accuracy`)%>%
  slice(1)

datasets[[2]][["rf"]]$predictions%>%table()
```

##Sensitivity
For sensitivity:
```{r}
results%>%
  arrange(-Sensitivity)%>%
  slice(1)

datasets[[2]][["rf"]]$predictions%>%table()
```

## Specificity
For Specificity
```{r}
results%>%
  arrange(-Specificity)%>%
  slice(1)

datasets[[5]][["xgb"]]$predictions%>%table()
```

## Plot

```{r}
results%>%
  gather("Metric", "value", 3:14)%>%
  ggplot(aes(y=Metric,x=Section))+
  geom_tile(aes(fill=value))+
  facet_wrap(Model~.)+
  scale_fill_viridis()+
  theme_minimal()+
  labs(title = "Final Model Prediction Metrics",
       caption = "Section: Portion of 30 minute window\n\"Section 7\" refers to HF_4 and LF_3")+
  scale_x_continuous(breaks = 1:7, labels = 1:7)
```

