---
title: "All the Models!"
output: html_notebook
---

Now that we know how to model urges using heart rate, we can compute models for each participant and for the global dataset.

```{r, include=FALSE}
#Libraries and functions

library(tidyverse)
library(nnet)
library(RWeka)
WPM("refresh-cache")
WPM("package-info", "repository", "naiveBayesTree")
WPM("install-package", "naiveBayesTree")
WOW("weka/classifiers/trees/NBTree")
NBTree = make_Weka_classifier("weka/classifiers/trees/NBTree")
library(ada)
library(e1071)
library(randomForest)
library(caret)
library(pander)
library(ROCR)
library(data.table)

source("HR_Feature_Engineering.R")
set.seed(5)
```

```{r, include=FALSE}
#load the data and create a list of participants

#get the IBI
path_to_file = "/Users/ttapera/Documents/QPS Local/Emote/Participants/tempAll_HR.csv"
f = fread(path_to_file, header=T, sep = ",")%>%
  as.data.frame()

ids = unique(f$Participant_ID)
participant_ids = paste0("p", ids)
parts = list()

for(x in 1:length(participant_ids)){
  parts[[participant_ids[x]]] = f%>%
    filter(Participant_ID == ids[x])%>%
    group_by(Session_Start)%>%
    arrange(Session_Start)
  
  t0 = parts[[participant_ids[x]]]$Beat[1]
  parts[[participant_ids[x]]]$Beat2 = parts[[participant_ids[x]]]$Session_Start - t0 + parts[[participant_ids[x]]]$Beat
}

#get lapses
path_to_file = "/Users/ttapera/Documents/QPS Local/Emote/Participants/tempLapses.csv"
parts$Lapses = fread(path_to_file, header=T, sep = ",")%>%
  as.data.frame()
parts$Lapses$Date = parts$Lapses$Lapse%>%
  as.POSIXct(origin = "1970-01-01", tz = "EST")
```

```{r, include=FALSE}
LONG = 50 #number of beats to calculate the updated mean
LAST = 13 #initial threshold
MIN_BPM = 25 #minimum physiologically acceptable value for HR
MAX_BPM = 200 #maximum physiologically acceptable value for HR

FOLD = 0.25 #portion of test data
WINDOW_SIZE = 300 #number of seconds in the sliding window
WINDOW_STEP = 5 #number of seconds to move the sliding window by
LOOKBACK = 60*30 #number of seconds in an observation (ie n seconds before the reported episode)

cols = c("hr","meanrr","sdnn","sdann","sdnnix", "pnn50", "sdsd", "rmssd", "irrr", "madrr")
participant_ids2 = c(participant_ids,"Global")
```

```{r, include=FALSE}
###############################################################################################
############################### Build the dataset ######################################

hrv = list()

for(p in 1:length(ids)){
  
  rr = preprocess_HR(parts[[participant_ids[p]]]$Beat2, parts[[participant_ids[p]]]$Session_Start[1], LONG, LAST, MIN_BPM, MAX_BPM)

  lapses = parts$Lapses%>%
    filter(Participant_ID == ids[p])
  
  lapse_df = data.frame(matrix(ncol = length(cols)))
  colnames(lapse_df) = cols
  for(x in 1:nrow(lapses)){
    dat = extract_features(rr, WINDOW_SIZE, WINDOW_STEP, lapses$Lapse[x], LOOKBACK)
    lapse_df = rbind(lapse_df, dat)
  }
  
  lapse_df = lapse_df[complete.cases(lapse_df),]
  lapse_df$Y = 1
  
  non_lapsedf = lapse_df[FALSE,]
  for(x in 1:nrow(lapses)){
    dat = find_nonlapse(rr, WINDOW_SIZE, WINDOW_STEP, lapses$Lapse[x], LOOKBACK)
    non_lapsedf = rbind(non_lapsedf, dat)
  }
  
  non_lapsedf = non_lapsedf[complete.cases(non_lapsedf),]
  non_lapsedf$Y = 0
  
  hrv[[p]] = partition(lapse_df,non_lapsedf,FOLD)

}

###############################################################################################
################################## Add Global data #########################################

train = NULL
test = NULL

for(x in 1:length(hrv)){
  train = rbind(train, hrv[[x]]$train)
  test = rbind(test, hrv[[x]]$test)
}

hrv[[length(participant_ids2)]] = list(train=train,test=test)

###############################################################################################
#################################### Modelling ###########################################

hrv_pred = lapply(hrv, function(x) build_models(x))

###############################################################################################
#################################### Modelling ###########################################

full_results = data.frame()
for(x in 1:length(hrv_pred)){
  Participant = participant_ids2[x]
  for(y in 1:length(hrv_pred[[x]])){
    Model = names(hrv_pred[[x]])[y]
    hrv_pred[[x]][[y]]$results = hrv_pred[[x]][[y]]$predictions%>%
      GetResults()
    full_results = data.frame(Participant, Model, hrv_pred[[x]][[y]]$results)%>%
      rbind(full_results,.)
  }
}
```

```{r, echo=FALSE}
df1 = full_results
models1 = hrv_pred
names(models1) = participant_ids2
```

# Interesting Queries

## What were the best overall predictions?

Here, we order the values prioritising `accuracy`, followed by `sensitivity` (ability to correctly detect an urge/episode), and lastly `specificity` (ability to correctly reject non-urges).

```{r, echo=FALSE}
df1%>%
  arrange(desc(Accuracy),desc(Sensitivity),desc(Specificity))%>%
  top_n(10, Accuracy)
```

From this, I wonder how we can decide the best model...

## What were the best predictions for each participant?

```{r, echo=FALSE}
df1%>%
  group_by(Participant)%>%
  arrange(Participant,desc(Accuracy),desc(Sensitivity),desc(Specificity))%>%
  top_n(1, Accuracy)
```

We can see that random forest is quite popular (even though its global prediction was low, as compared to this subset):

```{r, echo=FALSE}
df1%>%
  group_by(Participant)%>%
  arrange(Participant,desc(Accuracy),desc(Sensitivity),desc(Specificity))%>%
  top_n(1, Accuracy)%>%
  ungroup()%>%
  count(Model)%>%
  arrange(desc(n))
```

```{r, echo=FALSE}
df1%>%
  gather(Stat, Score, 3:5)%>%
  ggplot(aes(y = Score, x = Model))+
  geom_boxplot(aes(fill = Model))+
  #geom_jitter(aes(width = -0.6), alpha = 0.4)+
  coord_flip()+
  facet_grid(Stat~.)+
  ggtitle("Comparison of Model Performance on \nAccuracy, Sensitivity, & Specificity")
  
```

According to this plot, the random forest consistently performs best (judging by the mean). It is only outperformed in sensitivity, by the neural net (which has poorest mean accuracy and mean specificity) and the Naive Bayesian Tree (which also has poor mean specificity by comparison).

## So what did random forest do?

```{r, echo=FALSE}
df1%>%
  filter(Model == "RF")%>%
  arrange(desc(Accuracy),desc(Sensitivity),desc(Specificity))
```

Random forest was able to achieve 100% prediction results on participant 200, and performed poorly with participant 205.

## What makes participants 200 and 205 outliers in random forest's opinion?

First, we compare the model specifications:

```{r, echo=FALSE}
models1$p200$RF$object%>%
  varImpPlot(main = "Participant 200 Variable Importance Plot (Best Model)")
```

```{r, echo=FALSE}
models1$p205$RF$object%>%
  varImpPlot(main = "Participant 205 Variable Importance Plot (Worst Model)")
```

```{r, echo=FALSE}
models1$Global$RF$object%>%
  varImpPlot(main = "Global Model Variable Importance Plot (Average Model)")
```

By these plots, we can agree that the variables `sdnn`, `sdann`, and `sdnnix` are probably most important as they are found highest both in the global and best model. Let's confirm by ranking the variable importance in each model:

```{r, echo=FALSE}
fetchImp = function(list_el){
  importance(list_el$RF$object)%>%
    data.frame()%>%
    rownames_to_column("Variable")%>%
    arrange(desc(MeanDecreaseGini))%>%
    select(Variable)%>%
  return()
}

l=lapply(models1, function(x) fetchImp(x))%>%
  bind_cols()
names(l) = participant_ids2

for(c in 1:length(cols)){
  print(paste0("Ranks for variable `", cols[c], "`:"))
  
  which(l == cols[c], arr.ind = T)%>%
    data.frame()%>%
    count(row)%>%
    arrange(desc(n))%>%
    rename(Rank = row, n = n)%>%
    pander()
}
```

We can assume that participant 205 had the poorest predictions because their model selected poor variables. What do these variables look like?

Here's what the global data set looks like:

```{r, echo=FALSE}
hrv[[which(participant_ids2 == "Global")]]%>%
  bind_rows()%>%
  dim()

hrv[[which(participant_ids2 == "Global")]]%>%
  bind_rows()%>%
  mutate(Episodes = as.factor(Y), Y=NULL)%>%
  summary()
```

Here's participant 200:

```{r, echo=FALSE}
hrv[[which(participant_ids2 == "p200")]]%>%
  bind_rows()%>%
  dim()

hrv[[which(participant_ids2 == "p200")]]%>%
  bind_rows()%>%
  mutate(Episodes = as.factor(Y), Y=NULL)%>%
  summary()
```

And participant 205:

```{r, echo=FALSE}
hrv[[which(participant_ids2 == "p205")]]%>%
  bind_rows()%>%
  dim()

hrv[[which(participant_ids2 == "p205")]]%>%
  bind_rows()%>%
  mutate(Episodes = as.factor(Y), Y=NULL)%>%
  summary()
```

We can examine the distributions of the three top importance variables too:

```{r, echo=FALSE}
hrv[[which(participant_ids2 == "Global")]]%>%
  bind_rows()%>%
  select(sdnn:sdnnix)%>%
  gather(Variable, Value)%>%
  ggplot(aes(x=Value))+
  geom_density(aes(color = Variable))+
  theme_minimal()+
  ggtitle("Distribution of 3 Most Important Variables\nin the Global Model (Average Model)")
```

```{r, echo=FALSE}
hrv[[which(participant_ids2 == "p200")]]%>%
  bind_rows()%>%
  select(sdnn:sdnnix)%>%
  gather(Variable, Value)%>%
  ggplot(aes(x=Value))+
  geom_density(aes(color = Variable))+
  theme_minimal()+
  ggtitle("Distribution of 3 Most Important Variables\nin Participant 200's Model (Best Model)")
```

```{r, echo=FALSE}
hrv[[which(participant_ids2 == "p205")]]%>%
  bind_rows()%>%
  select(sdnn:sdnnix)%>%
  gather(Variable, Value)%>%
  ggplot(aes(x=Value))+
  geom_density(aes(color = Variable))+
  theme_minimal()+
  ggtitle("Distribution of 3 Most Important Variables\nin Participant 205's Model (Worst Model)")
```

Notice the difference in the ranges of these variables.

In any case, both participants 200 and 205 have extremely small data sets; changing any parameters such as the random seed could easily invalidate these results. Let's examine the second best predictions, participant 207 (who we examined before).

## Participant 207

```{r, echo=FALSE}
models1$p207$RF$object%>%
  varImpPlot(main = "Participant 207 Variable Importance\nPlot (Very Good Model)")
```

Once again, our top three variables are present.

```{r, echo=FALSE}
hrv[[which(participant_ids2 == "p207")]]%>%
  bind_rows()%>%
  select(sdnn:sdnnix)%>%
  gather(Variable, Value)%>%
  ggplot(aes(x=Value))+
  geom_density(aes(color = Variable))+
  theme_minimal()+
  ggtitle("Distributions of All Variables\nin Participant 207's Model (Very Good Model)")
```

I'm noticing that the good models all lack heavy tails. Participant 207's data is very similar to 200, however, it has a tiny bump way out at 400 for `sdnn`; by comparison, participant 200 had a range within 250 for these three variables. Does this mean that our preprocessing should apply a ceiling to these three variables?

## How do these important variables change for each event?

```{r, echo=FALSE}
fetchDFs = function(list_el){
  list_el%>%
    bind_rows()%>%
    data.frame()%>%
    return()
}

l2 = lapply(hrv, function(x) fetchDFs(x))[[length(participant_ids2)]]
l2%>%
  select(one_of(c("sdnn", "sdann", "sdnnix", "Y")))%>%
  group_by(Y)%>%
  mutate(Avg_sdnn = mean(sdnn),
         Avg_sdann = mean(sdann),
         Avg_sdnnix = mean(sdnnix))%>%
  select(Y:Avg_sdnnix)%>%
  distinct()%>%
  gather(Variable, Value, 2:4)%>%
  ggplot(aes(x=Variable, y=Value))+
  geom_bar(aes(fill=as.factor(Y)), stat = "identity", position="dodge")+
  #geom_text(aes(label = round(Value,3), x = Variable, y = Value), position = position_dodge(width = 0.8), vjust = -0.6)+
  scale_fill_discrete(name="Outcome",
                         breaks=c(0,1),
                         labels=c("Normal", "Episode/Urge"))+
  ggtitle("Comparing Our Variables of Interest Before An Episode/Urge\nand During Normal Activity")+
  theme_minimal()
```

## How are the decision trees structured?

Random forest is a simple algorithm with a complex explanation; it's considered one of the more *black box* algorithms, as it's not common to get too much detail about mechanics out of it. Nevertheless, we can derive some information by randomly selecting a tree and checking it's mechanics. This is not how the overall model works though (the real algorithm draws samples repeatedly and builds on the errors of small decision trees).

```{r, warning=FALSE, echo=FALSE}
for(pp in 1:length(models1)){
  tree_func(models1[[pp]]$RF$object, sample.int(1:models1[[pp]]$RF$object$ntree,1), participant_ids2[pp])
}
```

